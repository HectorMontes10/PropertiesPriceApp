{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "from time import time\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Import custom class and functions\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../app/customized_class\")\n",
    "from input_data import InputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function load a database of cleaned properties and remove non informative variables like this:\n",
    "    \n",
    "    -'l2', 'l3', 'l4', 'l5', 'l6', 'Region'\n",
    "    -'missing_l2', missing_l3', 'missing_l4', 'missing_l5', 'missing_l6'\n",
    "    \n",
    "    - 'l2' is removed because is redundant with 'l2shp'\n",
    "    - 'Region' is removed because a department(l2shp) belongs to a single region, therefore the department defines the region,\n",
    "       and this can lead to collinearity problems.\n",
    "    - 'missing_l2' and 'missing_price' are removed because are constant.(no missing values in this columns)\n",
    "    - lat and lon are in this dataframe but no in the model. They are used for visualizations.\n",
    "    \n",
    "    In addition to this we also remove values for properties other than houses or apartments, because the model\n",
    "    only include this categories.\n",
    "    \n",
    "    Params:\n",
    "        database_filepath (string): Path to sqlLite database\n",
    "    Returns:\n",
    "        df(pandas DataFrame): Matrix with features for train model and visualizations (lat and lon columns) and\n",
    "                              target column ('price')\n",
    "        \n",
    "    '''\n",
    "    engine = create_engine('sqlite:///'+database_filepath)\n",
    "    df = pd.read_sql_table(\"Cleaned_prices\",con=engine)\n",
    "    \n",
    "    columns_to_drop = ['l2', 'l3', 'l4', 'l5', 'l6','Region','missing_l2','missing_l3', 'missing_l4',\n",
    "                       'missing_l5', 'missing_l6', 'missing_price']\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    df = df[df['property_type'].isin(['Casa','Apartamento'])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def adjust_data_for_model(df):\n",
    "    \n",
    "    '''\n",
    "    This function the data in convenient format for the stage modelling. Some operations made are:\n",
    "    \n",
    "        1. Remove incomplete rows, that is, rows which have more than 2 missing fields in this list: \n",
    "           [rooms, log_surface_total, log_surface_covered, bathrooms]\n",
    "        2. Exclude departments with less of 100 rows in the dataframe\n",
    "        3. Include dummy variables for categorical variables: property_type, and l2shp (Department)\n",
    "           using One-Hot Encoding because they are nominal variables. Here the original categorical variables\n",
    "           are droped, except for l2shp because is ussefull for input median in missing values in a posterior step.\n",
    "        4. Replace price for log10(price).\n",
    "        5. Split the dataframe en covariates and target variable (X,y)\n",
    "        \n",
    "        \n",
    "    Parameters:\n",
    "    -----------\n",
    "        df(pandas DataFrame): DataFrame with relevant columns and rows for modelling stage\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "        \n",
    "        df(pandas DataFrame): DataFrame with features adjusted for modelling stage\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Step 1: Remove incomplete rows:\n",
    "    \n",
    "    columns = ['missing_rooms', 'missing_surface_total', 'missing_surface_covered','missing_bathrooms']\n",
    "    counts = df[columns].apply(sum,axis=1)\n",
    "    df = df[counts<=2]\n",
    "\n",
    "    # Step 2: Exclude departments with less of 100 points.\n",
    "    \n",
    "    rows_by_departments = df['l2shp'].value_counts()\n",
    "    departments_to_exclude = list(rows_by_departments[rows_by_departments<100].index)\n",
    "    df = df[~df['l2shp'].isin(departments_to_exclude)]\n",
    "    \n",
    "    # Step 3: Include dummy variables:\n",
    "    \n",
    "    var_cat = df.select_dtypes(include=['object']).copy().columns\n",
    "    for col in var_cat:\n",
    "        try:\n",
    "            \n",
    "            if col!='l2shp':\n",
    "                df = pd.concat([df.drop(col,axis=1),pd.get_dummies(df[col], prefix = col, prefix_sep = \"_\", drop_first = True, \n",
    "                                                                   dummy_na = False, dtype=int)],axis=1)\n",
    "            else:\n",
    "                df = pd.concat([df,pd.get_dummies(df[col], prefix = col, prefix_sep = \"_\", drop_first = True, \n",
    "                                                                   dummy_na = False,dtype=int)],axis=1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(col, \"processing error\")\n",
    "            print(e)\n",
    "        \n",
    "    # Step 4. Replace price for log10(price):\n",
    "    \n",
    "    df['price'] = np.log10(df['price'])\n",
    "    \n",
    "    # Step 5. Split the dataframe en covariates and target variable (X,y)\n",
    "    \n",
    "    X = df.loc[:,df.columns!=\"price\"]\n",
    "    y = df['price']\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"../data/PropertiesPrices.db\")\n",
    "display(df)\n",
    "df, y = adjust_data_for_model(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['l2shp'].value_counts())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = InputData()\n",
    "df_mod = transformer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_mod)\n",
    "print(df_mod.dtypes)\n",
    "display(transformer.medians_by_department)\n",
    "display(quantile98_by_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    '''\n",
    "    This function construct a pipeline with custom transformer and estimators. The pipeline is passed to grid search function\n",
    "    for tuning parameter for estimators. The pipeline include FeatureUnion based in custom transformer.\n",
    "    \n",
    "    Params:\n",
    "        None\n",
    "    Returns:\n",
    "        cv(GridSearch object): An object of class GridSearch fitting over train data. The object have an attribute \"best_estimator_\"\n",
    "                               that contain the best model finded.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "             ('input', InputData()),\n",
    "             ('scaler', StandardScaler()),\n",
    "             ('clf', DummyEstimator())])\n",
    "#   \n",
    "#   \n",
    "#    pipeline = Pipeline([\n",
    "#            ('transformer', Pipeline([\n",
    "#                ('input', InputData()),\n",
    "#                ('scaler', StandardScaler())\n",
    "#            ])),\n",
    "#            ('clf', DummyEstimator())\n",
    "#    ])\n",
    "    \n",
    "    \n",
    "   # pipeline = Pipeline([\n",
    "   #     ('features', FeatureUnion([\n",
    "   #         ('input', InputData()),\n",
    "   #         ('scaler', StandardScaler())\n",
    "   #     ])),\n",
    "   #     ('clf', DummyEstimator())\n",
    "   # ])\n",
    "\n",
    "    print(pipeline.get_params())\n",
    "    \n",
    "    # Estimator 1: LinearRegression (clasic model):\n",
    "    \n",
    "    fit_intercept = [False, True] \n",
    "    \n",
    "    # Estimator 2: Stochastic Gradient Descent:\n",
    "\n",
    "    # The gradient of the loss is estimated each sample at a time and the model is updated along the way with\n",
    "    # a decreasing strength schedule (aka learning rate). \n",
    "    \n",
    "    # Choosen loss functions for SGD\n",
    "    \n",
    "    loss_function_SGD =[\"squared_loss\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "    \n",
    "    # Epsilon parameter according loss function selected:\n",
    "    \n",
    "    epsilon_huber = [0.4,0.7,1]\n",
    "    epsilon_epsilon_insensitive = [0.01,0.1,0.2]\n",
    "    epsilon_squared_epsilon_insensitive = [0.01,0.1,0.2]\n",
    "    learning_rate = [\"invscaling\", \"adaptive\"]\n",
    "    \n",
    "    # Estimator 3: Support Vector Regression with Linear Kernel\n",
    "    \n",
    "    # Analogously to SVM for classification problem, the model produced by Support Vector Regression depends only\n",
    "    # on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.\n",
    "        \n",
    "    loss_functions_SVR = [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "    \n",
    "    # Candidate learning algorithms and their hyperparameters\n",
    "    \n",
    "    # Note that the SGDRegressor is splitted in several versions because loss functions is related to specific epsilon\n",
    "    # values\n",
    "\n",
    "    search_space = [{'clf': [LinearRegression()],\n",
    "                    'clf__fit_intercept': fit_intercept},\n",
    "                    {'clf': [SGDRegressor()],\n",
    "                     'clf__loss': ['squared_loss']},\n",
    "                    {'clf': [SGDRegressor()],\n",
    "                     'clf__loss': ['huber'],\n",
    "                     'clf__epsilon': epsilon_huber,\n",
    "                     'clf__learning_rate': learning_rate},\n",
    "                    {'clf': [SGDRegressor()],\n",
    "                     'clf__loss': ['epsilon_insensitive'],\n",
    "                     'clf__epsilon': epsilon_epsilon_insensitive,\n",
    "                     'clf__learning_rate': learning_rate},\n",
    "                    {'clf': [SGDRegressor()],\n",
    "                     'clf__loss': ['squared_epsilon_insensitive'],\n",
    "                     'clf__epsilon': epsilon_squared_epsilon_insensitive,\n",
    "                     'clf__learning_rate': learning_rate},\n",
    "                    {'clf': [LinearSVR()],\n",
    "                     'clf__loss': loss_functions_SVR}\n",
    "                   ]\n",
    "\n",
    "    #Create grid search\n",
    "\n",
    "    cv = GridSearchCV(pipeline, search_space, n_jobs=-1)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_data(\"../data/PropertiesPrices.db\")\n",
    "#df, y = adjust_data_for_model(df)\n",
    "\n",
    "X, y = df, y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 10)\n",
    "\n",
    "# The model will be trained without l2shp, lat and lon variables, these are dropped. \n",
    "# They were necessary up to this point for display purposes.\n",
    "\n",
    "X_train_mod = X_train.drop(['lat','lon'], axis=1) \n",
    "\n",
    "display(X_train_mod)\n",
    "\n",
    "print('Building model...')\n",
    "model = build_model()\n",
    "\n",
    "print('Training model...')\n",
    "start_time = time()\n",
    "model.fit(X_train_mod, y_train)\n",
    "end_time = time()\n",
    "print(\"The time for training was: {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.best_estimator_\n",
    "print(best_model)\n",
    "model_filepath = \"regressor.pkl\"\n",
    "pickle.dump(model, open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(model.cv_results_)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best model to predict prices in test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mod = X_test.drop(['lat','lon'], axis=1)\n",
    "y_pred = best_model.predict(X_test_mod)\n",
    "df_pred = pd.DataFrame({'y_pred':list(y_pred)}, index=X_test.index)\n",
    "df_results = pd.concat([X_test,df_pred,y_test],axis=1)\n",
    "df_results['errors'] = df_results['y_pred']-df_results['price']\n",
    "df_results['l2shp'] = df['l2shp']\n",
    "df_results['squared_errors'] = df_results['errors']**2\n",
    "df_results['missing_lon'] = df['missing_lon']\n",
    "df_results['missing_lat'] = df['missing_lat']\n",
    "df_to_save = df_results[['lat','lon','l2shp','errors','squared_errors','missing_lon','missing_lat']]\n",
    "display(df_to_save)\n",
    "df_to_save.to_csv(\"test_errors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_pj",
   "language": "python",
   "name": "capstone_pj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
